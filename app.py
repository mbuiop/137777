from flask import Flask, request, jsonify, redirect, render_template_string, session
from textblob import TextBlob
import nltk
import json
import hashlib
from difflib import SequenceMatcher
from nltk.tokenize import word_tokenize, sent_tokenize
from werkzeug.utils import secure_filename
import os
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from datetime import datetime
import requests
from bs4 import BeautifulSoup
import time
import PyPDF2
import docx
import csv
import re
from functools import wraps
import logging
from logging.handlers import RotatingFileHandler
import uuid
import random
from urllib.parse import quote

app = Flask(__name__)
app.secret_key = os.urandom(24)

# ØªÙ†Ø¸ÛŒÙ… Ù„Ø§Ú¯ÛŒÙ†Ú¯
if not os.path.exists('logs'):
    os.makedirs('logs')
    
handler = RotatingFileHandler('logs/app.log', maxBytes=10000000, backupCount=3)
handler.setFormatter(logging.Formatter('%(asctime)s %(levelname)s: %(message)s [in %(pathname)s:%(lineno)d]'))
handler.setLevel(logging.INFO)
app.logger.addHandler(handler)
app.logger.setLevel(logging.INFO)

# Ø¯Ø§Ù†Ù„ÙˆØ¯ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ NLTK
def download_nltk_data():
    required_packages = ['punkt', 'stopwords', 'averaged_perceptron_tagger', 'punkt_tab']
    for package in required_packages:
        try:
            nltk.download(package, quiet=True)
            app.logger.info(f"âœ… NLTK package {package} downloaded")
        except Exception as e:
            app.logger.error(f"âŒ Failed to download {package}: {e}")

download_nltk_data()

# ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø¨Ø±Ù†Ø§Ù…Ù‡
app.config['UPLOAD_FOLDER'] = 'uploads'
app.config['MAX_CONTENT_LENGTH'] = 500 * 1024 * 1024
app.config['MAX_HISTORY'] = 1000
app.config['SIMILARITY_THRESHOLD'] = 0.6

ALLOWED_EXTENSIONS = {'txt', 'pdf', 'docx', 'csv', 'json'}
os.makedirs('uploads', exist_ok=True)
os.makedirs('backups', exist_ok=True)

# ==================== Ø¯ÛŒØªØ§Ø¨ÛŒØ³ Ø­Ø§ÙØ¸Ù‡ ====================
knowledge_base = {}
file_history = []
chat_history = []
search_cache = {}

# ==================== ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø§Ù…Ù†ÛŒØªÛŒ ====================
ADMIN_USERNAME = "admin"
ADMIN_PASSWORD = "admin123"
ADMIN_PASSWORD_HASH = hashlib.sha256(ADMIN_PASSWORD.encode()).hexdigest()

def check_auth(username, password):
    return username == ADMIN_USERNAME and hashlib.sha256(password.encode()).hexdigest() == ADMIN_PASSWORD_HASH

def authenticate():
    return jsonify({'success': False, 'message': 'Ø§Ø­Ø±Ø§Ø² Ù‡ÙˆÛŒØª Ø§Ù„Ø²Ø§Ù…ÛŒ Ø§Ø³Øª', 'login_required': True}), 401

def requires_auth(f):
    @wraps(f)
    def decorated(*args, **kwargs):
        auth = request.authorization
        if not auth or not check_auth(auth.username, auth.password):
            return authenticate()
        return f(*args, **kwargs)
    return decorated

# ==================== ØªÙˆØ§Ø¨Ø¹ Ú©Ù…Ú©ÛŒ ====================
def allowed_file(filename):
    return '.' in filename and filename.rsplit('.', 1)[1].lower() in ALLOWED_EXTENSIONS

def safe_file_read(filepath, mode='r', encoding='utf-8', max_size=10*1024*1024):
    try:
        if os.path.getsize(filepath) > max_size:
            raise ValueError(f"ÙØ§ÛŒÙ„ Ø¨Ø²Ø±Ú¯ØªØ± Ø§Ø² Ø­Ø¯ Ù…Ø¬Ø§Ø² {max_size/1024/1024}MB Ø§Ø³Øª")
        with open(filepath, mode, encoding=encoding) as f:
            return f.read()
    except Exception as e:
        app.logger.error(f"Ø®Ø·Ø§ Ø¯Ø± Ø®ÙˆØ§Ù†Ø¯Ù† ÙØ§ÛŒÙ„ {filepath}: {e}")
        return ""

def read_txt_file(filepath):
    return safe_file_read(filepath)

def read_pdf_file(filepath):
    text = ""
    try:
        with open(filepath, 'rb') as f:
            pdf_reader = PyPDF2.PdfReader(f)
            for page_num, page in enumerate(pdf_reader.pages):
                try:
                    text += page.extract_text() + "\n"
                except Exception as e:
                    app.logger.warning(f"Ø®Ø·Ø§ Ø¯Ø± ØµÙØ­Ù‡ {page_num}: {e}")
                    continue
        return text
    except Exception as e:
        app.logger.error(f"Ø®Ø·Ø§ Ø¯Ø± Ø®ÙˆØ§Ù†Ø¯Ù† PDF: {e}")
        return ""

def read_docx_file(filepath):
    try:
        doc = docx.Document(filepath)
        return "\n".join([para.text for para in doc.paragraphs])
    except Exception as e:
        app.logger.error(f"Ø®Ø·Ø§ Ø¯Ø± Ø®ÙˆØ§Ù†Ø¯Ù† DOCX: {e}")
        return ""

def read_csv_file(filepath):
    text = ""
    try:
        with open(filepath, 'r', encoding='utf-8') as f:
            csv_reader = csv.reader(f)
            for row in csv_reader:
                text += " ".join(row) + "\n"
        return text
    except Exception as e:
        app.logger.error(f"Ø®Ø·Ø§ Ø¯Ø± Ø®ÙˆØ§Ù†Ø¯Ù† CSV: {e}")
        return ""

def extract_sentences(text):
    try:
        sentences = sent_tokenize(text)
        return [s.strip() for s in sentences if len(s.strip()) > 10]
    except:
        return [line.strip() for line in text.split('\n') if len(line.strip()) > 10]

def clean_text(text):
    text = re.sub(r'\s+', ' ', text)
    text = re.sub(r'[^\w\sØŸ?!.ØŒ]', '', text)
    return text.strip()

def extract_keywords(text, max_keywords=10):
    try:
        words = word_tokenize(text)
        stop_words = set(nltk.corpus.stopwords.words('persian') + nltk.corpus.stopwords.words('english'))
        keywords = [word.lower() for word in words if word.isalnum() and word.lower() not in stop_words and len(word) > 2]
        return list(set(keywords))[:max_keywords]
    except Exception as e:
        app.logger.error(f"Ø®Ø·Ø§ Ø¯Ø± Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ú©Ù„Ù…Ø§Øª Ú©Ù„ÛŒØ¯ÛŒ: {e}")
        return []

# ==================== Ú©Ù„Ø§Ø³ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø³ÙˆØ§Ù„ Ùˆ Ø¬ÙˆØ§Ø¨ ====================
class QuestionAnswerExtractor:
    def __init__(self):
        self.question_patterns = [
            r'[ØŸ?]\s*$',
            r'^(Ú†ÛŒØ³Øª|Ú©ÛŒØ³Øª|Ú©Ø¬Ø§Ø³Øª|Ú†Ø±Ø§|Ú†Ø·ÙˆØ±|Ú†Ú¯ÙˆÙ†Ù‡|Ø¢ÛŒØ§|Ú†Ù‡|Ú©Ø¯Ø§Ù…|Ú†Ù†Ø¯|Ú†Ù‡\s+Ú©Ø³ÛŒ|Ú†Ù‡\s+Ú†ÛŒØ²ÛŒ)',
            r'(Ú†ÛŒØ³Øª|Ú©ÛŒØ³Øª|Ú©Ø¬Ø§Ø³Øª|Ú†Ø±Ø§|Ú†Ø·ÙˆØ±|Ú†Ú¯ÙˆÙ†Ù‡|Ø¢ÛŒØ§|Ú†Ù‡|Ú©Ø¯Ø§Ù…|Ú†Ù†Ø¯)\s*$',
            r'\b(what|who|where|why|how|when|which|is|are|can|could)\b.*\?$',
        ]
        self.answer_indicators = [
            r'^Ù¾Ø§Ø³Ø®:',
            r'^Ø¬ÙˆØ§Ø¨:',
            r'^answer:',
            r'^Ù†ØªÛŒØ¬Ù‡:',
            r'^\d+[\.\)]',
            r'^[â€¢\-*]',
        ]
        self.stop_phrases = [
            'Ú©Ù¾ÛŒ Ø±Ø§ÛŒØª', 'ØªÙ…Ø§Ù…ÛŒ Ø­Ù‚ÙˆÙ‚', 'Ù…Ù†Ø¨Ø¹:', 'Ù…Ø±Ø¬Ø¹:', 'www.', 'http',
            'Ø¨Ø±Ø§ÛŒ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø¨ÛŒØ´ØªØ±', 'Ø§Ø¯Ø§Ù…Ù‡ Ù…Ø·Ù„Ø¨', 'Ù…Ù†Ø¨Ø¹ ØªØµÙˆÛŒØ±'
        ]
    
    def is_question(self, text):
        text = text.strip().lower()
        if len(text) < 5 or len(text) > 500:
            return False
        for pattern in self.question_patterns:
            if re.search(pattern, text, re.IGNORECASE):
                return True
        question_words = ['Ú†ÛŒØ³Øª', 'Ú©ÛŒØ³Øª', 'Ú©Ø¬Ø§Ø³Øª', 'Ú†Ø±Ø§', 'Ú†Ø·ÙˆØ±', 'Ú†Ú¯ÙˆÙ†Ù‡', 'Ø¢ÛŒØ§', 
                         'Ú†Ù‡', 'Ú©Ø¯Ø§Ù…', 'Ú†Ù†Ø¯', 'Ù…ÛŒØ´Ù‡', 'Ù…ÛŒØªÙˆØ§Ù†', 'Ù‡Ø³Øª', 'Ø¢ÛŒØ§']
        if any(word in text for word in question_words):
            return True
        if '?' in text or 'ØŸ' in text:
            return True
        return False
    
    def is_answer(self, text, min_length=20, max_length=2000):
        text = text.strip()
        if len(text) < min_length or len(text) > max_length:
            return False
        if any(phrase in text.lower() for phrase in self.stop_phrases):
            return False
        sentences = sent_tokenize(text)
        if len(sentences) < 2:
            return False
        return True
    
    def find_best_answer(self, lines, question_index, max_distance=5):
        best_answer = None
        best_score = 0
        for i in range(1, min(max_distance + 1, len(lines) - question_index)):
            candidate = lines[question_index + i].strip()
            if not candidate:
                continue
            if self.is_question(candidate):
                break
            score = 0
            if 50 <= len(candidate) <= 1000:
                score += 20
            elif len(candidate) > 1000:
                score += 10
            sentence_count = len(sent_tokenize(candidate))
            score += sentence_count * 5
            score += (max_distance - i) * 3
            if re.match(r'^[0-9]+[\.\)]', candidate):
                score += 15
            if candidate[0] in ['-', 'â€¢', '*']:
                score += 10
            if candidate[0].isupper():
                score += 5
            if score > best_score:
                best_score = score
                best_answer = candidate
        return best_answer, best_score
    
    def extract_qa_pairs(self, text, filename=""):
        lines = text.split('\n')
        qa_pairs = []
        i = 0
        app.logger.info(f"Ø´Ø±ÙˆØ¹ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ø² {filename} Ø¨Ø§ {len(lines)} Ø®Ø·")
        while i < len(lines):
            line = lines[i].strip()
            if not line or len(line) < 10:
                i += 1
                continue
            if self.is_question(line):
                app.logger.debug(f"Ø³ÙˆØ§Ù„ Ù¾ÛŒØ¯Ø§ Ø´Ø¯ Ø¯Ø± Ø®Ø· {i}: {line[:50]}...")
                answer, score = self.find_best_answer(lines, i)
                if answer and score > 30:
                    qa_pairs.append({
                        'question': line,
                        'answer': answer,
                        'line_number': i,
                        'confidence': score,
                        'source': filename
                    })
                    app.logger.info(f"âœ… Ø¬ÙØª Ø³ÙˆØ§Ù„-Ø¬ÙˆØ§Ø¨ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø´Ø¯ Ø¨Ø§ Ø§Ù…ØªÛŒØ§Ø² {score}")
                    i += 1
                    continue
            i += 1
        return qa_pairs
    
    def extract_from_structured(self, text, filename=""):
        qa_pairs = []
        try:
            data = json.loads(text)
            if isinstance(data, list):
                for item in data:
                    if isinstance(item, dict):
                        question = item.get('question') or item.get('q') or item.get('Ø³ÙˆØ§Ù„')
                        answer = item.get('answer') or item.get('a') or item.get('Ø¬ÙˆØ§Ø¨') or item.get('response')
                        if question and answer:
                            qa_pairs.append({
                                'question': str(question),
                                'answer': str(answer),
                                'confidence': 100,
                                'source': filename,
                                'type': 'json_structured'
                            })
            elif isinstance(data, dict):
                for key, value in data.items():
                    if isinstance(value, str) and len(value) > 50:
                        if self.is_question(key):
                            qa_pairs.append({
                                'question': key,
                                'answer': value,
                                'confidence': 90,
                                'source': filename,
                                'type': 'json_dict'
                            })
        except json.JSONDecodeError:
            pass
        return qa_pairs
    
    def extract_from_csv(self, text, filename=""):
        qa_pairs = []
        try:
            lines = text.split('\n')
            for line in lines:
                parts = line.split(',')
                if len(parts) >= 2:
                    question = parts[0].strip()
                    answer = ','.join(parts[1:]).strip()
                    if len(question) > 10 and len(answer) > 20:
                        qa_pairs.append({
                            'question': question,
                            'answer': answer,
                            'confidence': 80,
                            'source': filename,
                            'type': 'csv'
                        })
        except Exception as e:
            app.logger.error(f"Ø®Ø·Ø§ Ø¯Ø± Ø§Ø³ØªØ®Ø±Ø§Ø¬ CSV: {e}")
        return qa_pairs

# ==================== Ú©Ù„Ø§Ø³ Ø¬Ø³ØªØ¬ÙˆÛŒ Ø¢Ù†Ù„Ø§ÛŒÙ† Ù‚ÛŒÙ…Øª ====================
class OnlinePriceSearcher:
    def __init__(self):
        self.user_agents = [
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        ]
        self.price_patterns = [
            r'(\d{1,3}(?:,\d{3})*(?:\.\d+)?)\s*(ØªÙˆÙ…Ø§Ù†|Ø¯Ù„Ø§Ø±|ÛŒÙˆØ±Ùˆ|Ø¯Ø±Ù‡Ù…|Ù„ÛŒØ±|Ù¾ÙˆÙ†Ø¯)',
            r'(\d+(?:\.\d+)?)\s*(?:Ø¯Ù„Ø§Ø±|ØªÙˆÙ…Ø§Ù†|\$|â‚¬|Â£)',
            r'Ù‚ÛŒÙ…Øª.*?(\d+(?:,\d{3})*(?:\.\d+)?)\s*(ØªÙˆÙ…Ø§Ù†|Ø¯Ù„Ø§Ø±)',
            r'Ù‡Ø±\s*(Ú¯Ø±Ù…|Ù…Ø«Ù‚Ø§Ù„|Ø§Ù†Ø³|Ø¹Ø¯Ø¯)\s*(\d+(?:,\d{3})*(?:\.\d+)?)\s*(ØªÙˆÙ…Ø§Ù†|Ø¯Ù„Ø§Ø±)'
        ]
        self.price_keywords = {
            'gold': ['Ø·Ù„Ø§', 'Ø·Ù„Ø§ÛŒ', 'Ø³Ú©Ù‡', 'Ø§Ù…Ø§Ù…ÛŒ', 'Ø¨Ù‡Ø§Ø± Ø¢Ø²Ø§Ø¯ÛŒ', 'Ø§Ù†Ø³', 'Ø§ÙˆÙ†Ø³'],
            'currency': ['Ø¯Ù„Ø§Ø±', 'ÛŒÙˆØ±Ùˆ', 'Ù¾ÙˆÙ†Ø¯', 'Ø¯Ø±Ù‡Ù…', 'Ù„ÛŒØ±', 'Ø§Ø±Ø²', 'ØµØ±Ø§ÙÛŒ'],
            'crypto': ['Ø¨ÛŒØªâ€ŒÚ©ÙˆÛŒÙ†', 'Ø¨ÛŒØª Ú©ÙˆÛŒÙ†', 'Ø§ØªØ±ÛŒÙˆÙ…', 'Ø¨Ø§ÛŒÙ†Ù†Ø³', 'ØªØªØ±', 'USDT'],
            'oil': ['Ù†ÙØª', 'Ø·Ù„Ø§ÛŒ Ø³ÛŒØ§Ù‡', 'Ø¨Ø±Ù†Øª', 'WTI'],
            'coin': ['Ø³Ú©Ù‡', 'Ù†ÛŒÙ… Ø³Ú©Ù‡', 'Ø±Ø¨Ø¹ Ø³Ú©Ù‡', 'Ø³Ú©Ù‡ Ú¯Ø±Ù…ÛŒ']
        }
        self.trusted_sources = [
            'tgju.org', 'bonbast.com', 'donya-e-eqtesad.com', 'eghtesadnews.com',
            'alanchand.com', 'bazar360.com', 'boursenews.ir', 'kitco.com',
            'goldprice.org', 'xe.com'
        ]
        self.cache = {}
        self.cache_duration = 1800
    
    def _get_random_user_agent(self):
        return random.choice(self.user_agents)
    
    def is_price_question(self, question):
        question = question.lower()
        price_keywords = ['Ù‚ÛŒÙ…Øª', 'Ú†Ù†Ø¯Ù‡', 'Ú†Ù‚Ø¯Ø±', 'Ù†Ø±Ø®', 'Ù‚ÛŒÙ…ØªØ´', 'Ø¨Ù‡Ø§ÛŒ', 'Ø§Ø±Ø²Ø´', 'Ú¯Ø±ÙˆÙ†', 'Ø§Ø±Ø²ÙˆÙ†']
        if not any(keyword in question for keyword in price_keywords):
            return False
        for category, items in self.price_keywords.items():
            if any(item in question for item in items):
                return True
        return False
    
    def extract_price_item(self, question):
        question = question.lower()
        if 'Ø·Ù„Ø§' in question:
            if 'Ø§Ù†Ø³' in question or 'Ø§ÙˆÙ†Ø³' in question:
                return {'type': 'gold', 'subtype': 'ounce', 'name': 'Ø§Ù†Ø³ Ø·Ù„Ø§'}
            elif 'Ù…Ø«Ù‚Ø§Ù„' in question:
                return {'type': 'gold', 'subtype': 'mithqal', 'name': 'Ù…Ø«Ù‚Ø§Ù„ Ø·Ù„Ø§'}
            else:
                return {'type': 'gold', 'subtype': 'gram', 'name': 'Ú¯Ø±Ù… Ø·Ù„Ø§ 18 Ø¹ÛŒØ§Ø±'}
        if 'Ø³Ú©Ù‡' in question:
            if 'Ø§Ù…Ø§Ù…ÛŒ' in question or 'Ø·Ø±Ø­ Ø¬Ø¯ÛŒØ¯' in question:
                return {'type': 'coin', 'subtype': 'emami', 'name': 'Ø³Ú©Ù‡ Ø§Ù…Ø§Ù…ÛŒ'}
            elif 'Ø¨Ù‡Ø§Ø±' in question or 'Ø·Ø±Ø­ Ù‚Ø¯ÛŒÙ…' in question:
                return {'type': 'coin', 'subtype': 'bahar', 'name': 'Ø³Ú©Ù‡ Ø¨Ù‡Ø§Ø± Ø¢Ø²Ø§Ø¯ÛŒ'}
            elif 'Ù†ÛŒÙ…' in question:
                return {'type': 'coin', 'subtype': 'nim', 'name': 'Ù†ÛŒÙ… Ø³Ú©Ù‡'}
            elif 'Ø±Ø¨Ø¹' in question:
                return {'type': 'coin', 'subtype': 'rob', 'name': 'Ø±Ø¨Ø¹ Ø³Ú©Ù‡'}
            elif 'Ú¯Ø±Ù…ÛŒ' in question:
                return {'type': 'coin', 'subtype': 'grami', 'name': 'Ø³Ú©Ù‡ Ú¯Ø±Ù…ÛŒ'}
        if 'Ø¯Ù„Ø§Ø±' in question:
            return {'type': 'currency', 'subtype': 'usd', 'name': 'Ø¯Ù„Ø§Ø± Ø¢Ù…Ø±ÛŒÚ©Ø§'}
        if 'ÛŒÙˆØ±Ùˆ' in question:
            return {'type': 'currency', 'subtype': 'eur', 'name': 'ÛŒÙˆØ±Ùˆ'}
        if 'Ù¾ÙˆÙ†Ø¯' in question:
            return {'type': 'currency', 'subtype': 'gbp', 'name': 'Ù¾ÙˆÙ†Ø¯ Ø§Ù†Ú¯Ù„ÛŒØ³'}
        if 'Ø¯Ø±Ù‡Ù…' in question:
            return {'type': 'currency', 'subtype': 'aed', 'name': 'Ø¯Ø±Ù‡Ù… Ø§Ù…Ø§Ø±Ø§Øª'}
        if 'Ù„ÛŒØ±' in question:
            return {'type': 'currency', 'subtype': 'try', 'name': 'Ù„ÛŒØ± ØªØ±Ú©ÛŒÙ‡'}
        return None
    
    def search_google(self, query, num_results=3):
        try:
            headers = {
                'User-Agent': self._get_random_user_agent(),
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
                'Accept-Language': 'en-US,en;q=0.5,fa;q=0.3',
                'Accept-Encoding': 'gzip, deflate, br',
                'DNT': '1',
                'Connection': 'keep-alive',
                'Upgrade-Insecure-Requests': '1'
            }
            search_url = f"https://www.google.com/search?q={quote(query)}&num={num_results}"
            app.logger.info(f"ğŸ” Ø¬Ø³ØªØ¬Ùˆ Ø¯Ø± Ú¯ÙˆÚ¯Ù„: {query}")
            response = requests.get(search_url, headers=headers, timeout=10)
            response.raise_for_status()
            soup = BeautifulSoup(response.text, 'html.parser')
            results = []
            search_results = soup.find_all('div', class_='g') or soup.find_all('div', class_='rc')
            for result in search_results[:num_results]:
                title_elem = result.find('h3')
                link_elem = result.find('a')
                snippet_elem = result.find('div', class_='IsZvec') or result.find('span', class_='aCOpRe')
                if title_elem and link_elem:
                    title = title_elem.get_text()
                    link = link_elem.get('href', '')
                    if link.startswith('/url?q='):
                        link = link.split('/url?q=')[1].split('&')[0]
                    snippet = snippet_elem.get_text() if snippet_elem else ""
                    source_trust = any(source in link for source in self.trusted_sources)
                    results.append({
                        'title': title,
                        'link': link,
                        'snippet': snippet,
                        'trusted': source_trust
                    })
            app.logger.info(f"âœ… {len(results)} Ù†ØªÛŒØ¬Ù‡ Ø§Ø² Ú¯ÙˆÚ¯Ù„ Ø¯Ø±ÛŒØ§ÙØª Ø´Ø¯")
            return results
        except Exception as e:
            app.logger.error(f"Ø®Ø·Ø§ Ø¯Ø± Ø¬Ø³ØªØ¬ÙˆÛŒ Ú¯ÙˆÚ¯Ù„: {e}")
            return []
    
    def extract_price_from_snippet(self, snippet, item_info):
        if not snippet:
            return None
        prices = []
        for pattern in self.price_patterns:
            matches = re.findall(pattern, snippet, re.IGNORECASE)
            for match in matches:
                if isinstance(match, tuple):
                    price_num = match[0].replace(',', '')
                    price_unit = match[1]
                    try:
                        price_value = float(price_num)
                        if item_info['type'] == 'gold':
                            if item_info.get('subtype') == 'ounce' and 1000 < price_value < 10000:
                                prices.append(('Ø§Ù†Ø³', price_value, 'Ø¯Ù„Ø§Ø±'))
                            elif item_info.get('subtype') == 'gram' and 1000000 < price_value < 100000000:
                                prices.append(('Ú¯Ø±Ù…', price_value, 'ØªÙˆÙ…Ø§Ù†'))
                        elif item_info['type'] == 'currency':
                            if 50000 < price_value < 500000:
                                prices.append(('Ø¯Ù„Ø§Ø±', price_value, 'ØªÙˆÙ…Ø§Ù†'))
                    except ValueError:
                        continue
        return prices[0] if prices else None
    
    def get_price(self, question):
        item_info = self.extract_price_item(question)
        if not item_info:
            return None
        cache_key = f"{item_info['type']}_{item_info['subtype']}_{datetime.now().strftime('%Y-%m-%d')}"
        if cache_key in self.cache:
            cache_time, price_data = self.cache[cache_key]
            if (datetime.now() - cache_time).seconds < self.cache_duration:
                app.logger.info(f"âš¡ Ù‚ÛŒÙ…Øª Ø§Ø² Ú©Ø´: {item_info['name']}")
                return price_data
        search_query = f"Ù‚ÛŒÙ…Øª {item_info['name']} {datetime.now().strftime('%Y/%m/%d')}"
        results = self.search_google(search_query)
        if not results:
            return None
        best_price = None
        best_confidence = 0
        for result in results:
            price_info = self.extract_price_from_snippet(result['snippet'], item_info)
            confidence = 70 if result['trusted'] else 40
            if price_info and confidence > best_confidence:
                best_price = {
                    'item': item_info['name'],
                    'price': price_info[1],
                    'unit': price_info[2],
                    'source': result['link'],
                    'source_title': result['title'],
                    'confidence': confidence,
                    'timestamp': datetime.now().strftime('%H:%M:%S')
                }
                best_confidence = confidence
        if best_price:
            self.cache[cache_key] = (datetime.now(), best_price)
            return best_price
        return None
    
    def format_price_response(self, price_data):
        if not price_data:
            return "Ù…ØªØ§Ø³ÙØ§Ù†Ù‡ Ù†ØªÙˆÙ†Ø³ØªÙ… Ù‚ÛŒÙ…Øª Ø¯Ù‚ÛŒÙ‚ Ø±Ùˆ Ù¾ÛŒØ¯Ø§ Ú©Ù†Ù…. Ù„Ø·ÙØ§Ù‹ Ø¯Ù‚ÛŒÙ‚â€ŒØªØ± Ø¨Ù¾Ø±Ø³ÛŒØ¯."
        response = f"""ğŸ’° **{price_data['item']}**

Ù‚ÛŒÙ…Øª: {price_data['price']:,} {price_data['unit']}
â± Ø²Ù…Ø§Ù†: {price_data['timestamp']}
ğŸ“Š Ù…Ù†Ø¨Ø¹: {price_data['source_title'][:50]}

ğŸ”— {price_data['source']}"""
        return response

# ==================== ØªÙˆØ§Ø¨Ø¹ ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ ====================
def learn_from_text(question, answer, source='manual', confidence=100):
    question = clean_text(question)
    answer = clean_text(answer)
    if question in knowledge_base:
        old_version = knowledge_base[question].get('version', 1)
        knowledge_base[question]['answer'] = answer
        knowledge_base[question]['updated_at'] = str(datetime.now())
        knowledge_base[question]['version'] = old_version + 1
        knowledge_base[question]['confidence'] = confidence
        app.logger.info(f"ğŸ“ Ø¯Ø§Ù†Ø´ Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ø´Ø¯: {question[:30]}... (Ù†Ø³Ø®Ù‡ {old_version + 1})")
    else:
        knowledge_base[question] = {
            'answer': answer,
            'source': source,
            'learned_at': str(datetime.now()),
            'updated_at': str(datetime.now()),
            'usage': 0,
            'confidence': confidence,
            'version': 1,
            'keywords': extract_keywords(question + " " + answer),
            'answer_length': len(answer),
            'question_length': len(question)
        }
    auto_save()
    return True

def learn_from_file(filepath, filename):
    extracted = 0
    file_ext = filename.rsplit('.', 1)[1].lower()
    content = ""
    try:
        if file_ext == 'txt':
            content = read_txt_file(filepath)
        elif file_ext == 'pdf':
            content = read_pdf_file(filepath)
        elif file_ext == 'docx':
            content = read_docx_file(filepath)
        elif file_ext == 'csv':
            content = read_csv_file(filepath)
        elif file_ext == 'json':
            content = safe_file_read(filepath)
        else:
            return 0
        if not content:
            app.logger.warning(f"ÙØ§ÛŒÙ„ {filename} Ø®Ø§Ù„ÛŒ Ø§Ø³Øª")
            return 0
        app.logger.info(f"ğŸ“„ ÙØ§ÛŒÙ„ {filename} Ø¨Ø§ {len(content)} Ú©Ø§Ø±Ø§Ú©ØªØ± Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø´Ø¯")
        extractor = QuestionAnswerExtractor()
        all_qa_pairs = []
        if file_ext == 'json':
            qa_pairs = extractor.extract_from_structured(content, filename)
            all_qa_pairs.extend(qa_pairs)
        elif file_ext == 'csv':
            qa_pairs = extractor.extract_from_csv(content, filename)
            all_qa_pairs.extend(qa_pairs)
        qa_pairs = extractor.extract_qa_pairs(content, filename)
        all_qa_pairs.extend(qa_pairs)
        unique_pairs = {}
        for pair in all_qa_pairs:
            q_hash = hashlib.md5(pair['question'].encode()).hexdigest()
            if q_hash not in unique_pairs or pair['confidence'] > unique_pairs[q_hash]['confidence']:
                unique_pairs[q_hash] = pair
        for pair in unique_pairs.values():
            learn_from_text(pair['question'], pair['answer'], source=f'file:{filename}', confidence=pair.get('confidence', 70))
            extracted += 1
            app.logger.info(f"âœ… ÛŒØ§Ø¯ Ú¯Ø±ÙØªÙ…: {pair['question'][:30]}... (Ø§Ø¹ØªÙ…Ø§Ø¯: {pair.get('confidence', 70)}%)")
        file_history.append({
            'filename': filename,
            'extracted': extracted,
            'total_pairs': len(unique_pairs),
            'time': str(datetime.now()),
            'file_size': os.path.getsize(filepath)
        })
        if len(file_history) > app.config['MAX_HISTORY']:
            file_history[:] = file_history[-app.config['MAX_HISTORY']:]
        app.logger.info(f"ğŸ¯ Ù…Ø¬Ù…ÙˆØ¹ {extracted} Ù…ÙˆØ±Ø¯ ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ø§Ø² {filename}")
        return extracted
    except Exception as e:
        app.logger.error(f"Ø®Ø·Ø§ Ø¯Ø± ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ø§Ø² ÙØ§ÛŒÙ„ {filename}: {e}")
        return 0

def smart_search(user_question):
    price_searcher = OnlinePriceSearcher()
    if price_searcher.is_price_question(user_question):
        app.logger.info(f"ğŸ’° Ø³ÙˆØ§Ù„ Ù‚ÛŒÙ…ØªÛŒ ØªØ´Ø®ÛŒØµ Ø¯Ø§Ø¯Ù‡ Ø´Ø¯: {user_question}")
        price_data = price_searcher.get_price(user_question)
        if price_data:
            response = price_searcher.format_price_response(price_data)
            chat_history.append({
                'question': user_question,
                'answer': response,
                'type': 'online_price',
                'time': str(datetime.now())
            })
            return response
        else:
            return "Ù†ØªÙˆÙ†Ø³ØªÙ… Ù‚ÛŒÙ…Øª Ù„Ø­Ø¸Ù‡â€ŒØ§ÛŒ Ø±Ùˆ Ù¾ÛŒØ¯Ø§ Ú©Ù†Ù…. Ù„Ø·ÙØ§Ù‹ Ø¯Ù‚ÛŒÙ‚â€ŒØªØ± Ø¨Ù¾Ø±Ø³ÛŒØ¯ ÛŒØ§ Ø§Ø² Ù…Ù†Ø§Ø¨Ø¹ Ø¯ÛŒÚ¯Ù‡ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒØ¯."
    cache_key = hashlib.md5(user_question.encode()).hexdigest()
    if cache_key in search_cache:
        cache_time, answer = search_cache[cache_key]
        if (datetime.now() - cache_time).seconds < 300:
            app.logger.info(f"âš¡ Ù†ØªÛŒØ¬Ù‡ Ø§Ø² Ú©Ø´: {user_question[:30]}...")
            return answer
    best_match = None
    best_ratio = 0
    second_best = None
    second_ratio = 0
    for question in knowledge_base.keys():
        ratio1 = SequenceMatcher(None, user_question.lower(), question.lower()).ratio()
        q_keywords = set(extract_keywords(user_question))
        kb_keywords = set(knowledge_base[question].get('keywords', []))
        if q_keywords and kb_keywords:
            keyword_ratio = len(q_keywords & kb_keywords) / len(q_keywords) if q_keywords else 0
            ratio1 = (ratio1 + keyword_ratio) / 2
        if ratio1 > best_ratio:
            second_best, second_ratio = best_match, best_ratio
            best_ratio = ratio1
            best_match = question
        elif ratio1 > second_ratio:
            second_ratio = ratio1
            second_best = question
    threshold = app.config['SIMILARITY_THRESHOLD']
    if best_ratio > threshold:
        knowledge_base[best_match]['usage'] += 1
        answer = knowledge_base[best_match]['answer']
        if second_ratio > threshold and second_best:
            answer += f"\n\nğŸ’¡ Ù‡Ù…Ú†Ù†ÛŒÙ† Ù…ÛŒâ€ŒØªÙˆØ§Ù†ÛŒØ¯ Ø¨Ù¾Ø±Ø³ÛŒØ¯: {second_best}"
        search_cache[cache_key] = (datetime.now(), answer)
        chat_history.append({
            'question': user_question,
            'answer': answer,
            'type': 'knowledge_base',
            'time': str(datetime.now())
        })
        return answer
    no_answer = "Ù…ØªÙˆØ¬Ù‡ Ø³ÙˆØ§Ù„ØªÙˆÙ† Ù†Ø´Ø¯Ù…. Ù…ÛŒØ´Ù‡ ÙˆØ§Ø¶Ø­â€ŒØªØ± Ø¨Ù¾Ø±Ø³ÛŒØ¯ØŸ ÛŒØ§ Ù…ÛŒâ€ŒØªÙˆÙ†ÛŒØ¯ Ø§ÛŒÙ† Ù…Ø·Ù„Ø¨ Ø±Ùˆ Ø¨Ù‡ Ù…Ù† ÛŒØ§Ø¯ Ø¨Ø¯ÛŒØ¯."
    chat_history.append({
        'question': user_question,
        'answer': no_answer,
        'type': 'no_answer',
        'time': str(datetime.now())
    })
    return no_answer

def save_knowledge():
    with open('knowledge_base.json', 'w', encoding='utf-8') as f:
        json.dump(knowledge_base, f, ensure_ascii=False)

def load_knowledge():
    global knowledge_base
    try:
        with open('knowledge_base.json', 'r', encoding='utf-8') as f:
            knowledge_base = json.load(f)
    except:
        knowledge_base = {}

def auto_save():
    save_knowledge()
    print("âœ… Ø­Ø§ÙØ¸Ù‡ Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯")

# ==================== ØµÙØ­Ø§Øª Ø§ØµÙ„ÛŒ ====================
@app.route('/')
def home():
    return redirect('/chat')

# Ø§ÛŒÙ…Ù¾ÙˆØ±Øª ØµÙØ­Ø§Øª Ø§Ø² ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ø¯ÛŒÚ¯Ø±
from routes import *
from admin_routes import *

if __name__ == '__main__':
    load_knowledge()
    app.run(host='0.0.0.0', port=5000, debug=True)
